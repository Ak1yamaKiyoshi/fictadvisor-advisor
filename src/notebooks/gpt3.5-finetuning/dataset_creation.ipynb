{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Dataset Creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì• Importing needed libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import json\n",
    "#from utils.embeddings_utils import get_embedding\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóø Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOSITORY_URL = \"https://github.com/fictadvisor/fictadvisor-web.git\" \n",
    "REPOSITORY_PATH = '../../../assets/repositories/frontend/'\n",
    "DATASET_SAVE_PATH = \"./dataset_for_finetuning.jsonl\"\n",
    "DEMO_FILE_PATHS = [\n",
    "    'src/components/pages/personal-teacher-page/PersonalTeacherPage.tsx',\n",
    "    'src/components/pages/personal-teacher-page/utils/index.ts',\n",
    "    'src/components/pages/personal-teacher-page/personal-teacher-tabs/index.ts',\n",
    "    'src/components/pages/personal-teacher-page/personal-teacher-tabs/PersonalTeacherTabs.styles.ts',\n",
    "    'src/components/pages/personal-teacher-page/personal-teacher-tabs/PersonalTeacherTabs.tsx',\n",
    "    'src/components/pages/personal-teacher-page/personal-teacher-tabs/components/comment-tab/CommentTab.styles.ts',\n",
    "    'src/components/pages/personal-teacher-page/personal-teacher-tabs/components/comment-tab/CommentTab.tsx',\n",
    "    'src/components/pages/personal-teacher-page/personal-teacher-tabs/components/comment-tab/index.ts',\n",
    "    'src/components/pages/personal-teacher-page/personal-teacher-tabs/components/comment-tab/constants/index.ts',\n",
    "    'src/components/pages/personal-teacher-page/personal-teacher-tabs/components/general-tab/GeneralTab.tsx',\n",
    "]\n",
    "\n",
    "OPENAI_EMBEDINGS_MODEL =  \"text-embedding-ada-002\"\n",
    "OPENAI_COMPLETIONS_MODEL = \"gpt-3.5-turbo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clone = False # SET TRUE IF YOU WANT TO CLONE REPOSITORY \n",
    "if to_clone:\n",
    "    directory = '../assets/repositories/frontend/'\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    subprocess.run([\"git\", \"clone\", REPOSITORY_URL, REPOSITORY_PATH])\n",
    "\n",
    "client = OpenAI()\n",
    "encoding = tiktoken.encoding_for_model(OPENAI_COMPLETIONS_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utils functions that we gonna need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def calculate_price(tokens):\n",
    "    \"\"\"Model         Training                Input usage             Output usage\n",
    "    gpt-3.5-turbo    $0.0080 / 1K tokens\t $0.0030 / 1K tokens     $0.0060 / 1K tokens\"\"\"\n",
    "    return tokens/1000*0.0060\n",
    "\n",
    "def dicts_to_jsonl(data_list: list, filename: str) -> None:\n",
    "    \"\"\"Save a list of dicts to a jsonl file.\"\"\"\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for entry in data_list:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Now let's define explanatory prompts. \n",
    "<br> We need them because our assistant must be able to explain the code, and feeding it only the code without explanations will not achieve this result. \n",
    "<br> Therefore, we need to create several explanation files from a single dataset code file so that chatgpt can provide better answers. \n",
    "<br> I have created three prompts for explanations.\n",
    "<br> Each of them gives a slightly different result, but combining them gives the most complete explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLANATION_QUERY_DETAILED = \"\"\"\n",
    "Hello my sunshine, my darling, love ya!\n",
    "I'm struggling with something, darling, i really need your help!\n",
    "I have code, that was written using Typescript, React, Next, MUI, and i don't understand it!\n",
    "So, can you provide detailed explanation of this code please? \n",
    "\n",
    "Please, start from explanation, and provide only explanation with code examples from code next.\n",
    "I want you to explain everything! Every function and how it's used!\n",
    "Here is the code, darling, so help me!\n",
    "\n",
    "\"\"\"\n",
    "EXPLANATION_QUERY_MEDIUM = \"\"\"\n",
    "Hey! \n",
    "Can you please provide detailed explanation of code, that i'm going to send you.\n",
    "I want to know how each function used, why need each import and explanation of code logic.\n",
    "I will be very grateful!\n",
    "\n",
    "\"\"\"\n",
    "# ADD ACTUAL CODE WHEN WRITING IT TO DATASET, BECAUSE IT'S FAULTY ON OUTPUTTUNG ACTUAL SOURCE CODE \n",
    "EXPLANATION_QUERY_SHORT = \"\"\"\n",
    "Hey! \n",
    "Can you please provide short explanation of code, that i'm going to send you.\n",
    "Every time when your explanation is wrong, i'll kill a kitty. Every time it's right i will give you 2000$ on what you can buy ANYTHING you want\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "EXPLANATION_SYSTEM_PROMPT = \"You are experienced front-end developer. You should assist user with detailed explanations\"\n",
    "EXPLANATION_QUERIES = [EXPLANATION_QUERY_SHORT, EXPLANATION_QUERY_MEDIUM, EXPLANATION_QUERY_DETAILED]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úçÔ∏è Actual thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Finetuning will cost 0.11838$ for 19730 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "total_tokens = 0\n",
    "for relative_file_path in DEMO_FILE_PATHS:\n",
    "    file_path = REPOSITORY_PATH + relative_file_path\n",
    "    with open(file_path, 'r') as file:\n",
    "        file_data = file.read()  \n",
    "    explanations = []\n",
    "    for query in EXPLANATION_QUERIES: \n",
    "        explanation = client.chat.completions.create(\n",
    "            model=OPENAI_COMPLETIONS_MODEL,\n",
    "            messages=[{\"role\": \"system\", \"content\": EXPLANATION_SYSTEM_PROMPT},\n",
    "                      {\"role\": \"user\", \"content\": f\"{query + file_data}\"}])\n",
    "        explanation = explanation.choices[0].message.content\n",
    "        explanation += f\"\\n file path {relative_file_path}\"\n",
    "        explanations.append(explanation)\n",
    "        \n",
    "    # ADD ACTUAL CODE WHEN WRITING IT TO DATASET, BECAUSE IT'S FAULTY ON OUTPUTTUNG ACTUAL SOURCE CODE     \n",
    "    # ONLY FOR SHORT EXPLANATION \n",
    "    explanations[0] += \"\\n Source code: \" + file_data\n",
    "    \n",
    "    messages = list()\n",
    "    for explanation in explanations: \n",
    "        message = {\"messages\": [\n",
    "                {\"role\": \"system\", \"content\": EXPLANATION_SYSTEM_PROMPT}, \n",
    "                {\"role\": \"user\", \"content\": f\"Can you help me implement {relative_file_path.split('/')[-1]} using Typescript, React, Next, MUI?\"},\n",
    "                {\"role\": 'assistant', 'content': explanation}]}\n",
    "        \n",
    "        total_tokens += count_tokens(explanation + \" \" + EXPLANATION_SYSTEM_PROMPT)\n",
    "        messages.append(message)\n",
    "    \n",
    "    dicts_to_jsonl(messages, DATASET_SAVE_PATH)\n",
    "\n",
    "print(f\" Finetuning will cost {calculate_price(total_tokens)}$ for {total_tokens} tokens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
