{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Що ж. Для кінцевого результату нам важливо декілька речей\n",
    " - Те, як правильно модель буде розуміти поставлені задачі (bufix, enhancement, feature, etc.) \n",
    " - Планування - як добре модель може розписати покрокове рішення задачі \n",
    " - Написання коду - наскільки він валідний \n",
    "\n",
    "<br> Щоб досягти цього  існують декілька варіантів:\n",
    " 1. запхати весь код проєкту у модель та сподіватися на краще \n",
    " 2. розібрати декілька іш'юсів з покроковим поясненням кожної дії \n",
    "або ж обидва пункти одразу \n",
    "\n",
    "<br> Що важливо? \n",
    "1. Якщо модель не знатиме вже написаного коду, вона не зможе давати точні відповіді та вказувати на конкретні помилки / виправлення. Таким чином її функціонал обмежується до абстрактних порад на основі вже вивченого з інтернету коду. \n",
    "Це проблема, яку я намагаюсь вирішити, оскільки такий ефект рівносильний тому, щоб запхати код у чатгпт без будь-яких додаткових дій. \n",
    "\n",
    "2. У більшості випадків, модель так чи інакше не здатна вигадати геніальне рішення для нової фічі. ( хоча шаблонні рішення повинні даватись легко )\n",
    "З цим нічого не зробиш, оскільки у людського розробника досвіду куди більше, та людина врешті-решт може креативно мислити та комунікувати."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тим, як переходити до створення датасету, давайте проведемо невеличке дослідження, все ж, ми тут датасаєнсом займаємось! <br>\n",
    "З [треду](https://www.reddit.com/r/ChatGPT/comments/11ax0cy/is_it_possible_to_give_chatgpt_whole_of_my/) на реддіті, який чудово описує перше питання яке постало перед нами. \n",
    "<br>У мене вийшло дістати трошки корисної інформації, а саме те, що велика кількість коду не вміститься у короткостроковій пам'яті гпт, для цього існуює embedingsAPI\n",
    "<br>А також та знайшов [cookbook](https://cookbook.openai.com/examples/get_embeddings_from_dataset) яким демонструє як отримати embeddings з датасету "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Що ж таке ці embeddings, та з чого вони зліплені? [opeanAI](https://platform.openai.com/docs/guides/embeddings#:~:text=OpenAI's%20text%20embeddings%20measure%20the,related%20text%20strings%20are%20recommended)\n",
    "<br> Одразу відповідаючи на це питання: це векторна репрезентація слів, де вектор не тільки позначає саме по собі слово, а також його важливість та як воно відноситься до інших слів. \n",
    "<br> Те, як слова відносяться одне до одного визначити просто: це відстань між їх векторами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А тепер давайте спробуємо! \n",
    "<br> Для наглядності та економії часу у прикладі я буду використовувати реальний код проєкту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спочатку імпортуємо всі необхідні бібліотеки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "#from utils.embeddings_utils import get_embedding\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визначемо шлях завантаження, посилання на сам репозиторій та файл який ми будемо використовувати для прикладу \n",
    "<br> Зокрема зазначимо модель, яку будемо використовувати "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://github.com/fictadvisor/fictadvisor-web.git\" \n",
    "PATH = '../../../assets/repositories/frontend/'\n",
    "FILE = 'src/components/pages/personal-teacher-page/PersonalTeacherPage.tsx'\n",
    "MODEL =  \"text-embedding-ada-002\"\n",
    "GPTMODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clone = False # Щоб не клонувати кожен раз репозиторій, коли він вже є, \n",
    "if to_clone:\n",
    "    directory = '../assets/repositories/frontend/'\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    subprocess.run([\"git\", \"clone\", URL, PATH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаємо наш файл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Оскільки ми працюємо з трансформером, кожне слово повинно бути перетворене на токен ( у нашому випадку, у вигляді вектору ), оскільки мережі не розуміють слів напряму. \n",
    "<br> Існує багато способів токенізації, але зараз ми просто використаємо той, що пропонують самі openAI. \n",
    "<br> Чому це для нас взагалі важливо? \n",
    "<br> У кожної з запропонованих моделей на сьогоднішній день існує максимальна кількість токенів, що є для нас проблемою.\n",
    "<br> Але не тільки у цьому справа: за велику кількість токенів доведеться і більше платити :>\n",
    "\n",
    "<br> Годі слів, час пробувати!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для початку відкриємо не великий за змістом файл, скоріше серендій з оглядкою на мій досвід."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import {\n",
      "  createContext,\n",
      "  Dispatch,\n",
      "  FC,\n",
      "  SetStateAction,\n",
      "  useEffect,\n",
      "  useState,\n",
      "} from 'react';\n",
      "import { useRouter } from 'next/router';\n",
      "\n",
      "import Breadcrumbs from '@/components/common/ui/breadcrumbs';\n",
      "import PersonalTeacherCard from '@/components/common/ui/cards/personal-teacher-card';\n",
      "import Progress from '@/components/common/ui/progress';\n",
      "import PersonalTeacherTabs from '@/components/pages/personal-teacher-page/personal-teacher-tabs';\n",
      "import styles from '@/components/pages/personal-teacher-page/PersonalTeacherPage.module.scss';\n",
      "import {\n",
      "  PersonalTeacherPageProps,\n",
      "  TeachersPageTabs,\n",
      "} from '@/components/pages/personal-teacher-page/utils';\n",
      "import useTabState from '@/hooks/use-tab-state';\n",
      "import useToast from '@/hooks/use-toast';\n",
      "import { Teacher } from '@/types/teacher';\n",
      "\n",
      "// TODO: move context to separate folder, move types to separate folder\n",
      "export interface TeacherContext {\n",
      "  floatingCardShowed: boolean;\n",
      "  setFloatingCardShowed: Dispatch<SetStateAction<boolean>>;\n",
      "  teacher: Teacher;\n",
      "}\n",
      "\n",
      "export const teacherContext = createContext<TeacherContext>({\n",
      "  floatingCardShowed: false,\n",
      "  setFloatingCardShowed: () => {},\n",
      "  teacher: {} as Teacher,\n",
      "});\n",
      "\n",
      "const PersonalTeacherPage: FC<PersonalTeacherPageProps> = ({\n",
      "  isLoading,\n",
      "  isError,\n",
      "  data,\n",
      "  teacher,\n",
      "  query,\n",
      "  teacherId,\n",
      "}) => {\n",
      "  const router = useRouter();\n",
      "  const { push } = router;\n",
      "  const toast = useToast();\n",
      "  const [floatingCardShowed, setFloatingCardShowed] = useState(false);\n",
      "\n",
      "  const { tab } = query;\n",
      "\n",
      "  const [index, setIndex] = useState<TeachersPageTabs>(\n",
      "    TeachersPageTabs.GENERAL,\n",
      "  );\n",
      "\n",
      "  const handleChange = useTabState({ tab, router, setIndex });\n",
      "\n",
      "  useEffect(() => {\n",
      "    if (isError) {\n",
      "      toast.error('Куди ти лізеш, цієї людини не існує');\n",
      "      void push('/teachers');\n",
      "    }\n",
      "  }, [isError, push]);\n",
      "\n",
      "  if (!data) return null;\n",
      "  return (\n",
      "    <teacherContext.Provider\n",
      "      value={{\n",
      "        floatingCardShowed,\n",
      "        setFloatingCardShowed,\n",
      "        teacher,\n",
      "      }}\n",
      "    >\n",
      "      <div className={styles['personal-teacher-page']}>\n",
      "        {isLoading ? (\n",
      "          <div className={styles['personal-teacher-page-content']}>\n",
      "            <div className={styles['loader']}>\n",
      "              <Progress />\n",
      "            </div>\n",
      "          </div>\n",
      "        ) : (\n",
      "          !isError && (\n",
      "            <div className={styles['personal-teacher-page-content']}>\n",
      "              <Breadcrumbs\n",
      "                sx={{ margin: '16px 0px 16px 0px' }} //TODO move inline styles when refactor\n",
      "                items={[\n",
      "                  {\n",
      "                    label: 'Головна',\n",
      "                    href: '/',\n",
      "                  },\n",
      "                  { label: 'Викладачі', href: '/teachers' },\n",
      "                  {\n",
      "                    label: `${teacher.lastName} ${teacher.firstName} ${teacher.middleName}`,\n",
      "                    href: `/teachers/${teacherId}`,\n",
      "                  },\n",
      "                ]}\n",
      "              />\n",
      "              <div className={styles['card-wrapper']}>\n",
      "                <PersonalTeacherCard {...teacher} />\n",
      "              </div>\n",
      "              <div className={styles['tabs']}>\n",
      "                <PersonalTeacherTabs\n",
      "                  data={data}\n",
      "                  tabIndex={index}\n",
      "                  handleChange={handleChange}\n",
      "                  teacher={teacher}\n",
      "                />\n",
      "              </div>\n",
      "            </div>\n",
      "          )\n",
      "        )}\n",
      "      </div>\n",
      "    </teacherContext.Provider>\n",
      "  );\n",
      "};\n",
      "export default PersonalTeacherPage;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(PATH + FILE, 'r') as file:\n",
    "    filedata = file.read()\n",
    "print(filedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Та порахуємо скільки буде коштувати запхати його у нашу модель! ( багато ) ( бюджет СР не резиновий )\n",
    "<br> Для початку завантажимо токенайзер для нашої моделі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tiktoken' has no attribute 'encoding_for_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Downloading encodings for our model \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mencoding_for_models(MODEL)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tiktoken' has no attribute 'encoding_for_models'"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "# Downloading encodings for our model \n",
    "encoding = tiktoken.encoding_for_models(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_tokens(text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "count_tokens(filedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "744 Токени це велике число, не підйомне для мого гаманця, враховуючи що на проєкті fictadvisor більш ніж 950 файлів! \n",
    "<br> Давайде дізнаємось його ціну, можливо все не так погано"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744 tokens for 0.004464$\n"
     ]
    }
   ],
   "source": [
    "def calculate_price(tokens):\n",
    "    \"\"\" \n",
    "    Model            Training                Input usage             Output usage\n",
    "    gpt-3.5-turbo    $0.0080 / 1K tokens\t $0.0030 / 1K tokens     $0.0060 / 1K tokens\n",
    "    \"\"\"\n",
    "    return tokens/1000*0.0060\n",
    "\n",
    "print(f\"{count_tokens(filedata)} tokens for {calculate_price(count_tokens(filedata))}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отже, багатенько, як на один файл. Пропоную порахувати скільки коштувало б скормити весь код fictadvisor frontend нашій моделі! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 36302884\n",
      " total amount of files: 1024\n",
      " Average per file: 2.8207125362271494e-05\n",
      " Cost for all files: 217.817304$\n",
      " Cost per file: 0.2127122109375$\n"
     ]
    }
   ],
   "source": [
    "def tokens_in_each_file(directory):\n",
    "    total_tokens = 0\n",
    "    num_files = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                filedata = file.read()\n",
    "            tokens = count_tokens(filedata)\n",
    "            total_tokens += tokens\n",
    "            num_files += 1\n",
    "    return  {'total_tokens': total_tokens, 'total_files': num_files} \n",
    "\n",
    "\n",
    "data = tokens_in_each_file(PATH) \n",
    "print(\n",
    "    f\"Total tokens: {data['total_tokens']}\" \n",
    "    + f\"\\n Total amount of files: {data['total_files']}\" \n",
    "    + f\"\\n Average tokens per file: {data['total_tokens'] / data['total_files'] }\"\n",
    "    + f\"\\n Cost for all files: {calculate_price(data['total_tokens'])}$\"\n",
    "    + f\"\\n Cost per file: {calculate_price(data['total_tokens']) / data['total_files']}$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "217 Доларів, що ж, я не можу собі дозволити таке задоволення. Треба трошки подумати!\n",
    "<br> Перш за все, варто зауважити, що наведений код бере абсолютно всі файли які є у проєкті, це можуть бути і картинки також, а картинки можуть займати дуже багато місця\n",
    "<br> Окрім  того, код не прибирає імпорти, які є у кожному файлі, і їх на справді дуже багато. Для тренування це не така важлива інформація, а ось грошей тягне багато. \n",
    "<br> Далі: пробіли, код не розділяється на слова, тому зайві пробіли можуть бути токенізовані, цього теж слід уникати \n",
    "<br> Тому давайте перерахуємо вартість!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 332705\n",
      " Total amount of files: 897\n",
      " Average tokens per file: 370.9085841694537\n",
      " Cost for all files: 1.99623$\n",
      " Cost per file: 0.0022254515050167223$\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_tokens_per_file(directory):\n",
    "    total_tokens = 0\n",
    "    num_files = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            # skip non-js/ts files\n",
    "            if not (filename.endswith('.js') or filename.endswith('.jsx') or filename.endswith('.ts') or filename.endswith('.tsx')):\n",
    "                continue\n",
    "            \n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                filedata = file.readlines()\n",
    "            # skip imports and delete unnecessary spaces\n",
    "            \n",
    "            filedata = ''.join([line for line in filedata.split(\"\\n\") if 'import' not in line])\n",
    "            filedata = ' '.join(word.strip() for line in filedata.split() for word in line.split())\n",
    "            \n",
    "            tokens = count_tokens(filedata)\n",
    "            total_tokens += tokens\n",
    "            num_files += 1\n",
    "    return {'total_tokens': total_tokens, 'total_files': num_files}\n",
    "\n",
    "data = calculate_average_tokens_per_file(PATH) \n",
    "print(\n",
    "    f\"Total tokens: {data['total_tokens']}\" \n",
    "    + f\"\\n Total amount of files: {data['total_files']}\" \n",
    "    + f\"\\n Average tokens per file: {data['total_tokens'] / data['total_files'] }\"\n",
    "    + f\"\\n Cost for all files: {calculate_price(data['total_tokens'])}$\"\n",
    "    + f\"\\n Cost per file: {calculate_price(data['total_tokens']) / data['total_files']}$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Що ж, добре, той факт, що ми не розоримося на цьому проєкті не може не радувати.  \n",
    "<br> Давайте все-ж таки спробуємо зробити з цього embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для порівняння, попросимо зробити це завдання нашу модель до її файнтюнінгу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how do i do PersonalTeacherPage using React, Next, MUI for my website?\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{query}\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a personal teacher page for your website, follow these steps:\n",
      "\n",
      "1. Determine the purpose and content: Decide what information you want to include on your personal teacher page. Consider sharing your teaching philosophy, qualifications, experience, and any additional details that showcase your expertise and personality.\n",
      "\n",
      "2. Choose a website builder: Select a website builder or content management system (CMS) that suits your needs. Popular options include WordPress, Wix, Weebly, and Squarespace. Look for templates or themes that are suitable for personal portfolios or professional profiles.\n",
      "\n",
      "3. Register a domain name: Choose a domain name that reflects your name or teaching brand. Purchase your domain name from a domain registrar, such as GoDaddy or Namecheap.\n",
      "\n",
      "4. Set up hosting: Activate website hosting or choose a platform that offers hosting services. Many website builders provide built-in hosting, simplifying the process.\n",
      "\n",
      "5. Customize your website: Select a visually pleasing template or theme for your website. Modify colors, fonts, and layouts to match your preferences. Ensure that your website is easy to navigate, with clear menu options and organized sections.\n",
      "\n",
      "6. Create content: Start designing your personal teacher page. Include an introduction or welcome message, an overview of your teaching experience, details about your qualifications and certifications, your teaching philosophy, and any other relevant information. Consider incorporating testimonials or student feedback to further showcase your skills. Include professional photos and video snippets if available.\n",
      "\n",
      "7. Add contact information: Include your email address or a contact form on your website so that visitors can reach out to you. You might also consider adding links to your social media profiles, such as LinkedIn or Twitter.\n",
      "\n",
      "8. Implement user-friendly features: If possible, add interactive elements to your website, such as a calendar to display upcoming events or availability for tutoring. Include a section for frequently asked questions or additional resources that may be helpful to visitors.\n",
      "\n",
      "9. Optimize for search engines: Apply search engine optimization (SEO) techniques to increase the visibility of your website in search engine results. Research relevant keywords and use them naturally in your website's content, title tags, meta descriptions, and headings.\n",
      "\n",
      "10. Proofread and test: Ensure that all the information on your teacher page is accurate and up to date. Test your website on different devices and browsers to ensure that it is responsive and displays correctly.\n",
      "\n",
      "11. Publish and promote: Once you are satisfied with your website, publish it to make it live on the internet. Promote your website on social media, share it with colleagues, and add the link to your email signature to increase visibility.\n",
      "\n",
      "Remember to regularly update your personal teacher page with any new accomplishments, experiences, or changes to stay relevant and engaging to your visitors.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Щож, очікувано, що він не знає відповіді на це питання\n",
    "<br> Саме час це виправити!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "optimized = filedata\n",
    "optimized = ''.join([line for line in optimized.split(\"\\n\") if 'import' not in line])\n",
    "optimized = ' '.join(word.strip() for line in optimized.split() for word in line.split())\n",
    "\n",
    "sample = {\"messages\": \n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"\"}, \n",
    "    {\"role\": \"user\", \"content\": 'how do i do PersonalTeacherPage using React, Next, MUI for my website? Assume that i alrready have all needed components'},\n",
    "    {\"role\": 'assistant', 'content': optimized}\n",
    "]}\n",
    "\n",
    "with open('sample.json', 'w') as outfile:\n",
    "    json.dump(sample, outfile)\n",
    "\n",
    "dataset_filepath = 'sample.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-WNDMetlwt4l0wuD14QRZ9PwH', bytes=2554, created_at=1702586840, filename='sample.json', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file = client.files.create(\n",
    "  file=open(\"sample.json\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'invalid training_file: WNDMetlwt4l0wuD14QRZ9PwH', 'type': 'invalid_request_error', 'param': 'training_file', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m suffix_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfictadvisor-advisor-proof-of-concept\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m client\u001b[38;5;241m.\u001b[39mfine_tuning\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      3\u001b[0m   training_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWNDMetlwt4l0wuD14QRZ9PwH\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/fine_tuning/jobs.py:104\u001b[0m, in \u001b[0;36mJobs.create\u001b[0;34m(self, model, training_file, hyperparameters, suffix, validation_file, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m     51\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FineTuningJob:\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    Creates a job that fine-tunes a specified model from a given dataset.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/fine_tuning/jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    106\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    107\u001b[0m             {\n\u001b[1;32m    108\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    109\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: training_file,\n\u001b[1;32m    110\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyperparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: hyperparameters,\n\u001b[1;32m    111\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: suffix,\n\u001b[1;32m    112\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: validation_file,\n\u001b[1;32m    113\u001b[0m             },\n\u001b[1;32m    114\u001b[0m             job_create_params\u001b[38;5;241m.\u001b[39mJobCreateParams,\n\u001b[1;32m    115\u001b[0m         ),\n\u001b[1;32m    116\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    117\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    118\u001b[0m         ),\n\u001b[1;32m    119\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mFineTuningJob,\n\u001b[1;32m    120\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1086\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1074\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1082\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1083\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1084\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1085\u001b[0m     )\n\u001b[0;32m-> 1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:846\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    839\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    844\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    845\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    847\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    848\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    849\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    850\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    851\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    852\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:898\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m    896\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 898\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'invalid training_file: WNDMetlwt4l0wuD14QRZ9PwH', 'type': 'invalid_request_error', 'param': 'training_file', 'code': None}}"
     ]
    }
   ],
   "source": [
    "suffix_name = \"fictadvisor-advisor-proof-of-concept\"\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file='', \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
